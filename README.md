You're absolutely right â€” the **autoscaling logic** is mentioned in the features table, but not explicitly explained in the **architecture** or **system design narrative**. Let me update the relevant parts so it better reflects your effort on autoscaling.

---

### âœ… Here is the improved `README.md` with **autoscaling clearly emphasized**:

````markdown
# ğŸš¨ InferSafe

**InferSafe** is a production-grade, self-healing LLM inference infrastructure inspired by companies like Together.ai. Built with modularity, resilience, and scalability in mind, it supports:

- âš™ï¸ Dynamic batching
- ğŸ§  Multi-model inference
- ğŸ“‰ Token streaming
- ğŸ” Failure detection and auto-recovery
- ğŸ“ˆ Prometheus metrics for observability
- ğŸ“ˆ Simulated autoscaling logic (based on concurrency & load)
- ğŸš€ Kubernetes-ready architecture

---

## ğŸ”§ Architecture Overview

InferSafe is composed of:

- **FastAPI API Layer:** Entry point for all inference, reload, and metrics requests.
- **ModelWorker Instances:** Simulated LLMs (using TinyLLaMA or placeholders) with built-in retry and streaming.
- **MultiModelManager:** Handles GPU-aware routing and load balancing across multiple model workers.
- **Failure Detector:** Auto-heals failed models by reloading them and updating the worker pool.
- **Autoscaling Logic:** Monitors active request load and spins up/down worker instances based on thresholds (simulated).
- **Prometheus Metrics:** Exposes inference counts, latencies, retries, memory stats, and model health.

---

## ğŸ§ª Features Implemented

| Feature                          | Status |
|----------------------------------|--------|
| Dynamic Batching & Inference     | âœ…     |
| Token Streaming via Server-Sent Events | âœ… |
| Model Hot Reloading             | âœ…     |
| Multi-Model Load Balancing       | âœ…     |
| Simulated GPU-Aware Scheduling   | âœ…     |
| Failure Detection & Recovery     | âœ…     |
| Prometheus Observability         | âœ…     |
| Dockerized Environment           | âœ…     |
| Kubernetes YAML (basic setup)    | âœ…     |
| Horizontal Autoscaling (Simulated) | âœ…   |

---

## ğŸ“· Screenshots (Swagger UI)

> Demonstrating working endpoints:

- `/generate-batch`: Returns a generated response from the model.
- `/reload-model`: Dynamically reloads the model on failure.
- `/metrics`: Exposes Prometheus-compatible metrics.


## âš™ï¸ Simulated Autoscaling Logic

InferSafe simulates real-world autoscaling with:

- **Concurrency-aware scaling:** Spins up additional model workers when concurrent requests exceed a threshold.
- **Failure-aware fallback:** Automatically replaces failed workers and rebalances traffic.
- **Metrics-integrated decisions:** All scaling decisions are observable via Prometheus metrics (`active_workers`, `queued_requests`, etc.)

Although constrained by local resources, this logic mimics production Horizontal Pod Autoscalers (HPA) in Kubernetes.

---

## ğŸ“¦ Running the Project

### 1. Clone and Build the Docker Image

```bash
git clone https://github.com/yourusername/InferSafe.git
cd InferSafe
docker build -t infersafe .
````

### 2. Run the Container

```bash
docker run -p 8000:8000 infersafe
```

### 3. Visit Swagger Docs

Open your browser at `http://localhost:8000/docs`

---

## âš ï¸ Design Note on System Constraints

> This project was developed under limited local hardware constraints (8GB RAM, 4GB GPU), which restricted the use of large-scale LLMs or multi-GPU setups.
>
> As a result, the system **simulates**:
>
> * Multi-model workers
> * GPU-aware load balancing
> * Autoscaling decisions
>
> However, the architecture is fully modular and can plug directly into:
>
> * Real LLMs like LLaMA2, Mistral, or Mixtral
> * Actual GPU monitoring tools (like NVIDIA SMI)
> * Kubernetes Horizontal Pod Autoscaler (HPA) for scaling in production

ğŸ’¡ The core infrastructure is ready for production and only one deployment config away.

---

## ğŸ”® Future Improvements

* âœ… Integrate actual LLMs via HuggingFace Transformers
* â© Replace simulation with true GPU load metrics
* ğŸ“¦ Add Redis or Kafka for async job queues
* ğŸ“¡ Use HPA for true horizontal scaling on K8s
* ğŸŒ Add WebSocket support for real-time UI streaming

---

## ğŸ§  Why This Matters

InferSafe was built to showcase:

* Resilience engineering for AI systems
* MLOps principles applied to inference pipelines
* Autoscaling and recovery design under system constraints
* Real-world thinking in system design

---

## ğŸ“š Tech Stack

* `Python`, `FastAPI`, `Uvicorn`
* `Prometheus`, `Docker`
* `Kubernetes (basic YAMLs)`
* `pytest` for unit and load tests

---

## ğŸ‘¨â€ğŸ’» Author

**Hindol R. Choudhury @InferSafe**
*Built with â¤ï¸ 

---

## ğŸ“ License

MIT License

